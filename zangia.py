from src.service.zangia import get_all_data_and_save
from src.dependencies import get_zangia_sqlalchemy_repository, get_classifier_output_repository
from src.repositories.database import ZangiaJobRepository, JobClassificationOutputRepository

from schemas.base_classifier import JobClassificationInput, JobClassificationOutput, JobClassifierAgentConfig, JobClassifierAgent
from src.agent.agent import AgentProcessor
from typing import List

import asyncio
import json
from dotenv import load_dotenv
import os
from markdownify import markdownify as md

load_dotenv()

dep = get_zangia_sqlalchemy_repository()
dep_classifier_output = get_classifier_output_repository()

def test_get_all_data_and_save():
    """
    Test fetching all job listings from Zangia API and saving them to the database.
    """
    repository: ZangiaJobRepository = dep
    get_all_data_and_save(repository)

async def main():
    #get all data from database
    repository: ZangiaJobRepository = dep
    classifier_output_repository: JobClassificationOutputRepository = dep_classifier_output
    datas = repository.get_all()
    print(f"Total jobs in database: {len(datas)}")

    config = JobClassifierAgentConfig()
    agent = JobClassifierAgent(config=config)
    processor = AgentProcessor(agent)


    # I wanna batch requiest into 100 at a time, but for now let's just do one by one 
    # I 100 request batched then save result into database, but for now let's just do one by one


    #prepare data for classification
    prepared_data = []
    #ignore first 400 data for now, because I already classified them and saved into database

    for data in datas[2000:]:
        dict_data = data.__dict__
        classification_input = JobClassificationInput(
            job_title=dict_data.get("title", ""),
            job_description=md(dict_data.get("description", "")),
            company_name=dict_data.get("company_name", ""),
            salary_max=dict_data.get("salary_max", None),
            salary_min=dict_data.get("salary_min", None),
            additional_info={
                "responsibilities": md(dict_data.get("responsibilities", "")),
                "skills": ", ".join(dict_data.get("skills", [])),
                "tags": ", ".join(dict_data.get("tags", [])),
                "recruiter_industry": dict_data.get("recruiter_industry", ""),
            }
        )
        prepared_data.append((classification_input, dict_data.get("id")))

    #classify data batch that 100 by 100 and save result into database
    batch_size = 400
    counter = 0
    for i in range(0, len(prepared_data), batch_size):
        batch = prepared_data[i:i+batch_size]
        # 4 time parallel request with 50 batch size each
        tasks = []
        for j in range(0, len(batch), batch_size//8):
            sub_batch = batch[j:j+(batch_size//8)]
            tasks.append(asyncio.create_task(processor.process_batch([item[0] for item in sub_batch])))
        results = await asyncio.gather(*tasks)
        result = []

        for res in results:
            if res is not None:
                result.extend(res)

        if result is None:
            print(f"Batch {i//batch_size + 1}: Classification failed.")
            continue
        for output, (_, job_id) in zip(result, batch):
            # Create a new model instance (id will be auto-generated by database)
            output_dict = {
                "id": str(job_id),  # Use the same ID as the source job for traceability
                "title": output.title,
                "job_function": output.job_function,
                "job_industry": output.job_industry,
                "job_techpack_category": output.job_techpack_category,
                "job_level": output.job_level,
                "experience_level": output.experience_level,
                "salary_min": output.salary_min,
                "salary_max": output.salary_max,
                "education_level": output.education_level,
                "company_name": output.company_name,
                "requirement_reasoning": output.requirement_reasoning,
                "requirements": json.dumps([req.model_dump() for req in output.requirements], ensure_ascii=False),
                "benefits_reasoning": output.benefits_reasoning,
                "benefits": json.dumps([benefit.model_dump() for benefit in output.benefits], ensure_ascii=False),
                "confidence_scores": json.dumps(output.confidence_scores, ensure_ascii=False) if output.confidence_scores else None,
                "source_job": f"zangia"
            }

            classifier_output_repository.create(output_dict)


        print(f"Batch {i//batch_size + 1}: Classified and saved {len(batch)} jobs.")



    # Convert SQLAlchemy objects to dictionaries
   

if __name__ == "__main__":
    # test_get_all_data_and_save()
    asyncio.run(main())
